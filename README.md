Общее
=====

В данном репозитории находятся 4 работы: произведение матриц, сумма векторов,
расчеты числа Pi с помощью метода Монте Карло и билетеральный фильтр. Все работы
выполнены на языке Python с использованием функционала библиотеки Numba.

Основная работа проводилась в Google Collab. Расчеты велись на оборудовании
следующей конфигурации:

-   CPU: Intel Xeon \@ 2x 2.3GHz

-   GPU: Tesla K80

-   RAM: 12991MiB

-   OS: Ubuntu 18.04 bionic

 

Далее о всех четырех работах:

Произведение матриц
-------------------

В папке **MatMul** находятся два .ipynb файла. В файле
**MatrixMultiplication.ipynb** проводится сравнение скорости вычислений
умножения матриц на CPU и GPU. Основной переменной в этом сравнении выступает
*TBS* (аналог *TPB*. В следующих работах, я уже называл ее правильно).  Помимо
размера блока, эта переменная также учавствует в генерации количества элементов
матриц. Согласно лекциям, это позволяет получить наиболее эффективное
использование потоков в ядре, так как количество необходимых операций, кратно
размеру одного блока. Каждый цикл переменная *TBS* увеличивается на 1.

 

Результатом работы является график, на котором видно, что время выполнения на
GPU заметно быстрее, чем на CPU и с увелечением количества элементов - эта
разница увеличивается.

 

В файле **TPBmore32.ipynb** используется тот же код, но без вычислений на CPU.
Его цель была практическая проверка предела размера одного блока. В итоге, когда
переменная *TBS* , достигала значения 33, то код выдавал ошибку.

 

Сумма вектора
-------------

После импорта всех необходимых библиотек идет блок с тремя функциями. Первая

возвращает сумму элементов вектора, вторая и третья, по аналогии с первой

лабораторной работой, производят вычисления внутри видеокарты и копируют

необходимые данные для вычисления с хоста на девайс, возвращая искомую сумму.

 

Следующий блок представляет собой функцию \`\`\`main\`\`\`, в которую на входе

подается минимальное количество элементов вектора (начиная от 2), максимальное

количество элементов вектора и шаг на который количество элементов следующего

вектора будет отличаться от текущего. Для каждого сгенерированного вектора,

будет посчитана сумма его элементов, используя CPU и GPU. Вывод функции main для

каждого вектора представляет собой количество элементов вектора, время

вычисления на CPU, результат вычисления на CPU, время вычисления на GPU, время

вычисления на GPU, исключая время копирования данных на устройство, результат

вычислений на GPU. Все вышеуказанные данные сохраняются.

 

Слелующий блок графически представляет собранные данные. Изначально, я старался

сократить время вычисления на GPU, так как оно явно отставало от времени

вычисления на процессоре. Не добившись успехов, я решил измерить сколько

занимает время вычислений GPU без процесса копирования. Время вычислений на

самой GPU оказалось более быстрым для большого количества элементов, чем на CPU.

Было принято решение попробовать сократить время копирования данных. Как итог,

вместо изначальных трех входных данных на функцию dev_vec_sum (начальный вектор,

разбитый на два вектора с последующим суммированием его соответствующих

элементов и результирующий вектор) на функцию стало подаваться два элемента

(начальный вектор и результирующий), однако кроме более простого алгоритма

вычислений, ничего не изменилось. На самом графике видно скорость вычислений на

CPU (синий график), GPU (красный график) и вычисления на GPU без учета

копирования (синий график).

 

Расчет числа ПИ
---------------

С расчетами числа ПИ не возникло таких проблем, как с вычислением суммы вектора.
Была реализована функция для CPU, она же легла в основу расчета для GPU.
Единственной временной проблемой, была задача создания списков внутри
видеокарты. Решением оказался метод *xoroshiro128p_uniform_float32*, который
реализовал рандомную генерацию элемнетов массива. На выходе получены данные о
времени выполнения и точности выполнения для CPU и GPU, которые далее были
преобразованы в два графика. На первом видна эффективность видеокарты, по
сравнению с CPU для расчета от 0 до 1.000.000 миллионов элементов массива.
Второй график показался более интересным. Так как по сути метод расчета CPU и
GPU кардинально не отличался, то было сюрпризом увидеть разные результаты по
точности. Вычисления на GPU (синий график) выделяются своей большей
постоянностью результатов и в промежутке от 400.000 до примерно 700.000
элементов чуть более завышенными результатами числа ПИ. Результаты CPU же в свою
очередь отличаются большими колебаниями результата на протяжении всего
выполнения кода.

 

Билетеральный фильтр
--------------------

 

Билетеральный фильтр был реализован на подобие Гауссова размытия. Для каждого
пикселя счиаталсь среднее значение среди него и его соседних пикселей в кажом из
трех цветовых спектров. Только в отличие от гауссовского размытия, была введена
переменная *B*, которая отвечает за чувствительность фильтра. В эту переменную
указывается допустимая разница между значениями пикселей для их усреднения.
Таким образом значение этой переменной равное 0, скажет, что в усреднении кажого
пикселя допускаются только пиксели с таким же значением, а значение переменной
255 будет усреднять все пиксели, как это делает размытие Гаусса. В работе
реализовано билетеральное размытие с использованием видеокарты.

 
